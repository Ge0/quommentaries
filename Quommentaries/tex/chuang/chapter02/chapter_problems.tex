\subsection{Chapter 2 Problems}
\subsubsection{Problem 2.1}
This problem can be solved easily by combining the
logic of Exercises
\hyperref[sec:nielsen-and-chuang-exercise-2-35]{2.35} and
\hyperref[sec:nielsen-and-chuang-exercise-2-60]{2.60}.

It is known from exercise \hyperref[sec:nielsen-and-chuang-exercise-2-60]{2.60}
that $\vec{n} \vec{\sigma}$ has spectral decomposition
$+1P_+ - 1P_- = \\ +1 \left( \frac{I + \vec{n}\vec{\sigma}}{2} \right)
-1 \left( \frac{I - \vec{v}\vec{\sigma}}{2} \right)$.
Therefore, $\theta \vec{n} \vec{\sigma} =
\theta \left( \frac{I + \vec{n}\vec{\sigma}}{2} \right)
- \theta \left( \frac{I - \vec{v}\vec{\sigma}}{2} \right)$.
Then, by applying the definition of function operators and
the distributive property:
%
\begin{align}
    f(\theta \vec{n} \vec{\sigma}) &= f(\theta) \left(
        \frac{I + \vec{n}\vec{\sigma}}{2} \right) +
        f(-\theta) \left( \frac{I - \vec{v}\vec{\sigma}}{2} \right) \\
    &= \frac{f(\theta)}{2}I + \frac{f(-\theta)}{2}I +
        \frac{f(\theta)}{2}\vec{n}\vec{\sigma} -
        \frac{f(-\theta)}{2}\vec{n}\vec{\sigma} \\
    &= \frac{f(\theta) + f(-\theta)}{2}I +
        \frac{f(\theta) - f(-\theta)}{2}\vec{n}\vec{\sigma}
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Problem 2.2}

\begin{enumerate}
    \item Since $\ket{\psi}$ is pure, it follows from Theorem 2.7, that
    $\ket{\psi} = \sum_i \lambda_i \ket{i_A} \ket{i_B}$.
    In addition, $\rho^A = \sum_i \lambda_i^2 \ketbra{i_A}{i_A}$.
    From the rank-nullity theorem, it is known that
    $rank(A) + nullity(A) = dim(A)$ where $dim(A)$ is the dimension of the matrix $A$.
    Also, $rank(A) = dim(Col(A))$ where $dim(Col(A))$ is the dimension of the columnspan of $A$.
    Notwithstanding, $row(A) \cup kernel(A)$ spans $\mathbb{C}^n$, and is linearly independent.
    Thus, forming a basis for $\mathbb{C}^n$.

    To prove that $rank(\rho^A) = Sch(\ket{\psi})$,
    it is possible to use the rank-nullity theorem.
    Henceforth, find $kernel(\rho^A)$,
    that is, $\set{\ket{x} \in \mathbb{C}^n}$ such that $\rho^A \ket{x} = \vec{0}$.
    Assume the possibility that $\exists k, \lambda_k = 0$.
    Also, since $\ket{i_A}$ spans $\mathbb{C}^n$,
    any vector $\ket{x}$ can written as a linear combination of $\ket{i_A}$.
    Then,
    %
    \begin{align}
        &\left( \sum_i \lambda_i^2 \ketbra{i_A}{i_A} \right) \ket{x}
        = \left( \sum_i \lambda_i^2 \ketbra{i_A}{i_A} \right) \sum_j x_j \ket{j_A}
        = \sum_{ij} \lambda_i^2 x_j \ket{i_A} \braket{i_A}{j_A} \\
        %
        &= \sum_j \lambda_j^2 x_j \ket{j_A}
        = \sum_{j \neq k} \lambda_j^2 x_j \ket{j_A} + \sum_k \lambda_k^2 x_k \ket{k_A}
        = \sum_{j \neq k} \lambda_j^2 x_j \ket{j_A} + \sum_k 0 x_k \ket{k_A}
    \end{align}
    
    Therefore, $kernel(\rho^A) = \ket{k_A}$ where $\forall k, \lambda_k = 0$.
    As such, $Sch(\ket{\psi}) = n - dim(kernel(\rho^A)) = n - nullity(\rho^A)$.
    Then, using the rank-nullity theorem conclude that $rank(\rho^A) = Sch(\ket{\psi})$.

    \dotfill

    \item To prove that $j > Sch(\psi)$ note that if $j > 1$,
    then it is possible to obtain an orthonormal basis for $\ket{\psi}$
    such that $\ket{\psi}$ is equal to the tensor product of
    exactly one element of each subsystem's basis
    ($\ket{\psi} = \ket{a} \otimes \ket{b}$).
    
    To prove that $j = Sch(\psi)$, suppose that $\ket{\alpha_j}$ and $\ket{\beta_j}$
    are linearly independent (thus forming a basis for a $\ket{\psi}$).
    Then, obtain bases $\ket{a_i}$ and $\ket{b_i}$ through Gram-Schmidt.
    Therefore, $Sch(\psi) = i = j$.
    
    In addition, no restriction is given regarding the number of $j$ elements.
    Since they are not normalised, it is possible that
    $\ket{\alpha_j}$ and $\ket{\beta_j}$ are not linearly independent.
    Thus, by obtaining an orthonormal basis $\ket{a_i}$ and $\ket{b_i}$ from
    $\ket{\alpha_j}$ and $\ket{\beta_j}$ via Gram-Schmidt, $j > i$.
    Also proving that $j > Sch(\psi)$.
    
    \dotfill
    
    \item The solution when $\alpha = 0$ or $\beta = 0$ is trivial,
    since $Sch(\psi) = max(Sch(\varphi), Sch(\gamma))$.
    Henceforth, assume that $\alpha \neq 0$ and $\beta \neq 0$.
    Also, assume that $\ket{\varphi} \neq \mu \ket{\gamma}$,
    $\mu \in \mathbb{C}$.
    
    $\ket{\psi}$ is a pure state of $A$ and $B$,
    and it is a linear combination of $\ket{\varphi}$ and $\ket{\gamma}$.
    Also, note that $\ket{\psi}$, $\ket{\varphi}$, and $\ket{\gamma}$
    must be written in the same orthonormal basis
    $\ket{a_i}$ for subsystem $A$ and $\ket{b_i}$ analogously for $B$.
    Therefore,
    %
    \begin{align}
        \ket{\psi} &= \ket{\psi} \\
        \sum_i \lambda_i \ket{a_i} \ket{b_i} &= \alpha \ket{\varphi} + \beta \ket{\gamma} \\
        \sum_i \lambda_i \ket{a_i} \ket{b_i} &=
            \alpha \sum_i \varphi_i \ket{a_i} \ket{b_i} + \beta \sum_i \gamma_i \ket{a_i} \ket{b_i} \\
    \end{align}
    
    Where $\varphi_i, \gamma_i \in \mathbb{C}$ respectively are the indexes of the linear combinations
    of $\ket{\varphi}$ and $\ket{\gamma}$, according to the basis $\ket{a_i} \ket{b_i}$.
    Then,
    %
    \begin{align}
        \sum_i \lambda_i \ket{a_i} \ket{b_i} &=
            \sum_i (\alpha \varphi_i + \beta \gamma_i) \ket{a_i} \ket{b_i} \\
        \lambda_i &= \alpha \varphi_i + \beta \gamma_i
    \end{align}
    
    Using set theory, define $\Psi \equiv \set{i \mid \lambda_i \neq 0}$,
    $\Phi \equiv \set{i \mid \varphi_i \neq 0}$, and
    $\Gamma \equiv \set{i \mid \gamma_i \neq 0}$.
    Note that it is possible that $\alpha \varphi_i + \beta \gamma_i = 0$,
    thus, it is useful to define another set
    $\Delta \equiv \set{i \mid \alpha \varphi_i + \beta \gamma_i = 0
    \land \varphi_i \neq 0 \land \gamma_i \neq 0}$
    (where $\land$ indicates logical conjunction).
    Then, $Sch(\psi) = |\Psi|$, $Sch(\varphi) = |\Phi|$, and $Sch(\gamma) = |\Gamma|$.
    Therefore,
    %
    \begin{align}
        \Psi &= (\Phi \cup \Gamma) - \Delta \\
        |\Psi| &= |(\Phi \cup \Gamma)| - |\Delta|
    \end{align}
    
    Note that whenever
    $(\forall \varphi_i \neq 0, \alpha \varphi_i + \beta \gamma_i = 0) \lor
    (\forall \gamma_i \neq 0, \alpha \varphi_i + \beta \gamma_i = 0)$,
    then $max(|\Delta|) = min(|\Phi|, |\Gamma|)$,
    where $\lor$ indicates logical disjunction.
    Thus, $Sch(\psi) = |Sch(\varphi) - Sch(\gamma)|$.
    Also, $min(|\Delta|) = 0$ whenever
    $\forall \varphi_i \neq 0 \land \gamma_i \neq 0,\ \alpha \varphi_i + \beta \gamma_i \neq 0$.
    Thus, $Sch(\psi) > |Sch(\varphi) - Sch(\gamma)|$.
    Therefore, $Sch(\psi) \geq |Sch(\varphi) - Sch(\gamma)|$, as required.
    
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Problem 2.3}

$Q$, $R$, $S$, $T$ are unitary matrices.
Referring back to
\hyperref[sec:nielsen-and-chuang-equation-2-116]{Equation 2.116},
%
\begin{align}
    Q = \left[ \begin{matrix}
        q_3 & q_1 - q_2i \\
        q_1 + q_2i & - q_3
        \end{matrix} \right]
\end{align}

Note that $Q = Q^\dagger$ and that
%
\begin{align}
    Q Q^\dagger &= \left[ \begin{matrix}
        q_3 & q_1 - q_2i \\
        q_1 + q_2i & - q_3
        \end{matrix} \right]
        %
        \left[ \begin{matrix}
        q_3 & q_1 - q_2i \\
        q_1 + q_2i & - q_3
        \end{matrix} \right] \\[5pt]
    &= \left[ \begin{matrix}
        q_1^2 + q_2^2 + q_3^2 & q_3(q_1 - q_2i) - q_3(q_1 - q_2i) \\
        q_3(q_1 - q_2i) - q_3(q_1 - q_2i) & q_1^2 + q_2^2 + q_3^2
        \end{matrix} \right]
\end{align}

Since $\vec{q} \in \mathbb{R}^3$ is a unit vector,
$q_1^2 + q_2^2 + q_3^2 = 1$,
%
\begin{align}
    Q Q^\dagger = I
\end{align}

Thus proving that $Q$ is an unitary matrix
(and analogously for $R$, $S$, and $T$).

Calculating $(Q \otimes S + R \otimes S + R \otimes T - Q \otimes T)^2$,
and writing it as $M^2$
%
\begin{align}
    M^2 =
    &(Q \otimes S)(Q \otimes S) + (Q \otimes S)(R \otimes S) +
    (Q \otimes S)(R \otimes T) - (Q \otimes S)(Q \otimes T) +
    \nonumber \\
    & (R \otimes S)(Q \otimes S) + (R \otimes S)(R \otimes S) +
    (R \otimes S)(R \otimes T) - (R \otimes S)(Q \otimes T) +
    \nonumber \\
    &(R \otimes T)(Q \otimes S) + (R \otimes T)(R \otimes S) +
    (R \otimes T)(R \otimes T) - (R \otimes T)(Q \otimes T) -
    \nonumber \\
    &(Q \otimes T)(Q \otimes S) - (Q \otimes T)(R \otimes S) -
    (Q \otimes T)(R \otimes T) + (Q \otimes T)(Q \otimes T)
\end{align}

Using Equation 2.48,
%
\begin{align}
    M^2 =
    &QQ \otimes SS + QR \otimes SS + QR \otimes ST - QQ \otimes ST +
    \nonumber \\
    &RQ \otimes SS + RR \otimes SS + RR \otimes ST - RQ \otimes ST +
    \nonumber \\
    &RQ \otimes TS + RR \otimes TS + RR \otimes TT - RQ \otimes TT -
    \nonumber \\
    &QQ \otimes TS - QR \otimes TS - QR \otimes TT + QQ \otimes TT
\end{align}

Since $QQ^\dagger = I$ and $Q = Q^\dagger$, then $QQ = I$,
analogously for $R, S, T$.
%
\begin{align}
    M^2 = 
    &I \otimes I + QR \otimes I + QR \otimes ST - I \otimes ST +
    \nonumber \\
    &RQ \otimes I + I \otimes I + I \otimes ST - RQ \otimes ST +
    \nonumber \\
    &RQ \otimes TS + I \otimes TS + I \otimes I - RQ \otimes I -
    \nonumber \\
    &I \otimes TS - QR \otimes TS - QR \otimes I + I \otimes I
\end{align}

Refactoring,
%
\begin{alignat}{2}
    M^2 &= 
    &&4I \otimes I + (QR + RQ - RQ - QR) \otimes I +
    I \otimes (-ST + ST + TS - TS) +
    \nonumber \\
    & &&QR \otimes ST - RQ \otimes ST +
    RQ \otimes TS - QR \otimes TS
    \\
    &= && 4I + QR \otimes ST - RQ \otimes ST + RQ \otimes TS - QR \otimes TS
    \\
    &= && 4I + QR \otimes (ST - TS) - RQ \otimes (ST - TS)
    \\
    &= && 4I + (QR - RQ) \otimes (ST - TS)
\end{alignat}

Therefore, by the definition of commutator (Equation 2.66),
%
\begin{align}
    (Q \otimes S + R \otimes S + R \otimes T - Q \otimes T)^2 =
    4I + [Q, R] \otimes [S, T]
\end{align}

Recall that $Var(x) = E(x^2) - E(x)^2$ 
(as described in the book's Appendix 1).
Also, since $Var(x) \geq 0$,
\begin{align}
    E(x^2) - E(x)^2 &\geq 0 \\
    E(x)^2 &\leq E(x^2) \\
    E(x) &\leq \sqrt{E(x^2)}
\end{align}

Then, calculate $E(x^2)$.
Recall that $E(M) = \bra{\psi} M \ket{\psi} \equiv \mean{M}$
(as described in Equations 2.110 to 2.115).
Also, $E(M + N) = E(M) + E(N)$ and $E(MN) = E(M) E(N)$
Thus, 
\begin{align}
    E(M^2) &= \mean{M^2} \\
    &= \mean{4I + [Q, R] \otimes [S, T]} \\
    &= \mean{4I} + \mean{[Q, R] \otimes [S, T]}
\end{align}

Calculate the mean for $[Q, R]$. The mean of $[S, T]$ is analagous.
\begin{align}
    \mean{[Q, R]} &= \mean{QR - RQ} \\
    &= \mean{QR} - \mean{RQ} \\
    &= \bra{\psi}QR\ket{\psi} - \bra{\psi}RQ\ket{\psi} \\
    &= \bra{\psi}QR\ket{\psi} - \bra{\psi}R^\dagger Q^\dagger \ket{\psi} \\
    &= \bra{\psi}QR\ket{\psi} - \bra{\psi}(QR)^\dagger \ket{\psi} \\
    &= \bra{\psi}QR\ket{\psi} - (\bra{\psi} QR \ket{\psi})^\dagger
\end{align}

Since $\bra{\psi} QR \ket{\psi} \in \mathbb{C}$,
write the resulting complex number as
$\bra{\psi} QR \ket{\psi} = a + bi$,
where $a, b \in \mathbb{R}$.
Therefore,
\begin{align}
    \bra{\psi}QR\ket{\psi} - (\bra{\psi} QR \ket{\psi})^\dagger
    &= a + bi - (a + bi)* \\
    &= a + bi - (a - bi) \\
    &= 2bi \in [-2i, 2i]
\end{align}

Note that $QR$ is unitary
($(QR)^\dagger QR = R^\dagger Q^\dagger Q R = R^\dagger I R = I$).
As a consequence, the inner product between vectors is preserved.
Thus, from $\braket{\psi}{\psi}$,
it is possible to conclude that
$(a + bi)(a + bi)* = a^2 + b^2 = 1$.
As such, $-1 \leq b \leq 1$.
Back to the mean calculus,
and considering $\mean{\bra{\psi} ST \Ket{\psi} = c + di}$,
\begin{align}
    \mean{4I} + \mean{[Q, R] \otimes [S, T]} &=
        \bra{\psi} 4I \ket{\psi} + 2bi \cdot 2di \\
	&= 4 \braket{\psi}{\psi} + 4 bd i^2 \\
	&= 4 - 4 bd
\end{align}

Where $-1 \leq bd \leq 1$.
Therefore, the maximum value of $E(x^2)$ is 8 (whenever $bd = -1$).
Since $E(x) \leq \sqrt{E(x^2)}$,
\begin{align}
	\mean{Q \otimes S + R \otimes S + R \otimes T - Q \otimes T} &\leq
		\sqrt{4 - 4bd} \\
	&\leq 2 \sqrt2
\end{align}

As requested.